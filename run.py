#!/usr/bin/env python3
import os
import subprocess
import sys

def clear_screen():
    os.system('clear' if os.name != 'nt' else 'cls')

def show_menu():
    clear_screen()
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   ğŸ•·ï¸  WEB SCRAPER & API DISCOVERY TOOL ğŸ•·ï¸                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This powerful tool helps you:
  âœ… Scrape any website automatically
  âœ… Discover hidden API endpoints
  âœ… Capture all network traffic
  âœ… Save data in organized formats

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                              MAIN MENU                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  [1] ğŸŒ Scrape a Website (with browser visible)
  [2] ğŸ” Scrape a Website (headless mode - faster)
  [3] ğŸ“ View Previous Results
  [4] â„¹ï¸  Help & Examples
  [5] ğŸšª Exit

""")

def scrape_with_browser():
    clear_screen()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                     ğŸŒ SCRAPE WITH VISIBLE BROWSER                         â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    url = input("Enter the URL to scrape (e.g., https://example.com): ").strip()
    
    if not url:
        print("\nâŒ No URL provided!")
        input("\nPress Enter to continue...")
        return
    
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    print(f"\nğŸš€ Starting scrape of: {url}")
    print("ğŸ“º Browser window will open - please wait...\n")
    
    subprocess.run([sys.executable, "scraper.py", url])
    
    input("\n\nâœ… Press Enter to return to menu...")

def scrape_headless():
    clear_screen()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                       ğŸ” SCRAPE IN HEADLESS MODE                           â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    url = input("Enter the URL to scrape (e.g., https://example.com): ").strip()
    
    if not url:
        print("\nâŒ No URL provided!")
        input("\nPress Enter to continue...")
        return
    
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    print(f"\nğŸš€ Starting headless scrape of: {url}")
    print("âš¡ Running in background (faster)...\n")
    
    subprocess.run([sys.executable, "scraper.py", url, "--headless"])
    
    input("\n\nâœ… Press Enter to return to menu...")

def view_results():
    clear_screen()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                          ğŸ“ PREVIOUS RESULTS                               â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    if not os.path.exists('scrape_results'):
        print("âŒ No results folder found yet. Run a scrape first!\n")
        input("Press Enter to continue...")
        return
    
    files = os.listdir('scrape_results')
    
    if not files:
        print("ğŸ“­ No scraping results saved yet.\n")
        input("Press Enter to continue...")
        return
    
    api_files = [f for f in files if f.endswith('_apis.txt')]
    json_files = [f for f in files if f.endswith('.json')]
    
    print(f"ğŸ“Š Total files saved: {len(files)}\n")
    print(f"   â€¢ API endpoint files: {len(api_files)}")
    print(f"   â€¢ Complete data files: {len(json_files)}")
    print(f"   â€¢ HTML files: {len([f for f in files if f.endswith('.html')])}\n")
    
    if api_files:
        print("Latest API discoveries:\n")
        for f in sorted(api_files, reverse=True)[:5]:
            print(f"   ğŸ“„ {f}")
    
    print(f"\nğŸ’¡ All files are in the 'scrape_results' folder")
    
    input("\n\nPress Enter to continue...")

def show_help():
    clear_screen()
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                            â„¹ï¸  HELP & EXAMPLES                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ WHAT THIS TOOL DOES:

This tool automates the process of analyzing websites by:
  â€¢ Opening the website in a real browser (Chromium)
  â€¢ Recording ALL network traffic (images, scripts, API calls, etc.)
  â€¢ Identifying API endpoints automatically
  â€¢ Extracting page content and HTML
  â€¢ Saving everything in organized files

ğŸ” WHAT YOU GET:

After scraping, you'll find these files in the 'scrape_results' folder:

  1. [domain]_[timestamp]_apis.txt
     â†’ List of all discovered API endpoints with responses

  2. [domain]_[timestamp].json
     â†’ Complete data: all requests, responses, and metadata

  3. [domain]_[timestamp].html
     â†’ The full HTML content of the page

ğŸ“ EXAMPLE URLS TO TRY:

  â€¢ https://jsonplaceholder.typicode.com/
  â€¢ https://reqres.in/
  â€¢ Any e-commerce site to see their API calls
  â€¢ News websites to discover their content APIs

ğŸ’¡ TIPS:

  â€¢ Use "visible browser" mode to see what's happening
  â€¢ Use "headless mode" for faster scraping
  â€¢ The tool captures AJAX calls, fetch requests, and GraphQL
  â€¢ Look for 'application/json' responses to find APIs

âš ï¸  IMPORTANT:

  â€¢ Only scrape websites you have permission to access
  â€¢ Respect robots.txt and terms of service
  â€¢ Be mindful of rate limiting

""")
    input("Press Enter to return to menu...")

def main():
    while True:
        show_menu()
        choice = input("Select an option (1-5): ").strip()
        
        if choice == '1':
            scrape_with_browser()
        elif choice == '2':
            scrape_headless()
        elif choice == '3':
            view_results()
        elif choice == '4':
            show_help()
        elif choice == '5':
            clear_screen()
            print("\nğŸ‘‹ Thanks for using Web Scraper & API Discovery Tool!\n")
            sys.exit(0)
        else:
            print("\nâŒ Invalid option. Please choose 1-5.")
            input("Press Enter to continue...")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        clear_screen()
        print("\n\nğŸ‘‹ Goodbye!\n")
        sys.exit(0)
